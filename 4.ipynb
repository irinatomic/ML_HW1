{"cells":[{"cell_type":"markdown","metadata":{"id":"HXdaoHEOFPS3"},"source":["Use the [train.csv](https://www.kaggle.com/competitions/nlp-getting-started/data?select=train.csv) file that contains a set of tweets for classification. The file, detailed description of the data and the research objective can be found [here](https://www.kaggle.com/competitions/nlp-getting-started). Running this file should execute all the mentioned tasks in the problem statement and output all relevant results."]},{"cell_type":"markdown","metadata":{"id":"xXvwFviPFTbb"},"source":["## 4A\n","\n","Clean the dataset and then create feature vectors using chosen methods.\n","Split the dataset into training and test sets (in an 80:20 ratio). Fit a Multinomial Naive Bayes model. An accuracy of at least 65% on the test set is required (average over three consecutive program runs)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1712138303817,"user":{"displayName":"Irina Tomic","userId":"15842450607275704514"},"user_tz":-120},"id":"1OY1HZ4UFO0I","outputId":"9d2989dd-3c94-4d71-c932-ed4f2d3049bc"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.tokenize import wordpunct_tokenize\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import PorterStemmer, porter\n","from nltk.stem.porter import *\n","from nltk.stem import *\n","from nltk.corpus import stopwords\n","from nltk import FreqDist\n","\n","from string import punctuation\n","import pandas as pd\n","import numpy as np\n","import math\n","import random\n","import re\n","\n","from typing import List\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcN_E0jCeyA3"},"outputs":[],"source":["# Load a file with tweets\n","file_name = 'disaster-tweets.csv'\n","csvFile = pd.read_csv(file_name, usecols=['target', 'text'])\n","csvFile = csvFile.dropna()\n","\n","# Y - array of target values\n","Y = csvFile['target'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3528,"status":"ok","timestamp":1712143788920,"user":{"displayName":"Irina Tomic","userId":"15842450607275704514"},"user_tz":-120},"id":"-TU7XmxqFMCT","outputId":"703ee323-1c4c-4f4d-b986-0284d18a32b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating the clean_corpus\n","Creating the vocab...\n","broj tweetova: 7613\n","broj unikatnih reci: 13172\n"]}],"source":["# Stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Regexes\n","link_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n","word_with_number_regex = r'\\b\\w*\\d\\w*\\b'\n","remove_non_alpha_regex = r'[^a-zA-Z\\s]+'\n","username_regex = r'@[\\w]+'\n","emoji_regex = r'.*\\x89.*'                       # \\x89 - emojis\n","\n","combined_regex = f'({\"|\".join([link_regex, username_regex, word_with_number_regex, remove_non_alpha_regex, emoji_regex])})'\n","\n","def clean_words(words: List[str]):\n","\n","  lowered_words = [word.lower() for word in words]\n","  regexed_words = [word for word in lowered_words if word not in stop_words and not re.match(combined_regex, word)]\n","\n","  # interpunction and stopwords\n","  stop_punc = set(stopwords.words('english')).union(set(punctuation))\n","  no_punc_words = [word for word in regexed_words if word not in stop_punc]\n","\n","  long_words = [word for word in no_punc_words if len(word) >= 3]\n","\n","  # just in case\n","  cleaned_words = [w for w in long_words if w != 'the' and 'http' not in w]\n","  return cleaned_words\n","\n","\n","porter = PorterStemmer()\n","\n","# clean_corpus = list of clean tweets,\n","# tweet = array of clean words\n","\n","print('Creating the clean_corpus')\n","clean_corpus = []\n","for tweet in csvFile['text']:\n","  tokenized_words = wordpunct_tokenize(tweet)\n","  cleaned_words = clean_words(tokenized_words)\n","  stemmed_words = [porter.stem(word) for word in cleaned_words]\n","  clean_corpus.append(stemmed_words)\n","\n","# Vocab - al unique words\n","print('Creating the vocab...')\n","vocab_set = set()\n","for doc in clean_corpus:\n","  for word in doc:\n","    vocab_set.add(word)\n","vocab = list(vocab_set)\n","\n","print(f'broj tweetova: {len(clean_corpus)}')\n","print(f'broj unikatnih reci: {len(vocab)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110073,"status":"ok","timestamp":1712143903613,"user":{"displayName":"Irina Tomic","userId":"15842450607275704514"},"user_tz":-120},"id":"0fnFggF6Kj0T","outputId":"df1a8b03-fc62-4006-d154-a388dcd5f791"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating BOW features...\n","X shape: 7613x13172\n","X:\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"]}],"source":["# BOW - Bag of Words\n","\n","def freq_score(word, doc):\n","  return doc.count(word) / len(doc)\n","\n","print('Creating BOW features...')\n","\n","\"\"\"\n","X  is the feature vector, which will have the frequency of each word appearing in the tweet.\n","\n","tweet1 = 'ana ana voli milovana'\n","tweet2 = 'jabuke'\n","\n","vocab = {ana, danas, voli, jabuke, milovan, banane}\n","\n","[\n","[2/4, 0, 1/4, 0, 1/4, 0], # tweet1\n","[0, 0, 0, 1, 0, 0] # tweet2\n","]\n","\"\"\"\n","\n","X = np.zeros((len(clean_corpus), len(vocab)), dtype=np.float32)\n","for doc_idx in range(len(clean_corpus)):                                # doc_idx - tweet index\n","  doc = clean_corpus[doc_idx]                                           # doc - tweet\n","  for word_idx in range(len(vocab)):\n","    word = vocab[word_idx]\n","    cnt = freq_score(word, doc)\n","    X[doc_idx][word_idx] = cnt\n","\n","print(f'X shape: {len(X)}x{len(X[0])}')\n","print('X:')\n","print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154750,"status":"ok","timestamp":1712144064699,"user":{"displayName":"Irina Tomic","userId":"15842450607275704514"},"user_tz":-120},"id":"ZzDqoBpndmxf","outputId":"66b73fc0-5898-4fae-c821-4a34a0fb6b98"},"outputs":[{"name":"stdout","output_type":"stream","text":["Priors:\n","[0.57034021 0.42965979]\n","Occurences:\n","[[ 1.11697749 12.38168247  0.0625     ...  0.09090909  0.16666667\n","   1.40708183]\n"," [ 0.18333334  5.13586427  0.         ...  0.          0.21111111\n","   0.52454215]]\n","Likelihoods:\n","[[1.20873443e-04 7.64056321e-04 6.06657528e-05 ... 6.22878319e-05\n","  6.66133759e-05 1.37437583e-04]\n"," [7.19657807e-05 3.73159657e-04 6.08161525e-05 ... 6.08161525e-05\n","  7.36551182e-05 9.27167876e-05]]\n","Accuracy:  0.8023637557452397\n"]}],"source":["class MultinomialNaiveBayes:\n","  def __init__(self, nb_classes, nb_words, pseudocount):\n","    self.nb_classes = nb_classes\n","    self.nb_words = nb_words\n","    self.pseudocount = pseudocount\n","\n","  def fit(self, X, Y):\n","    nb_examples = X.shape[0]\n","\n","    # P(Class) - priors\n","    # np.bincount returns the number of occurrences of each integer\n","    # in the given list within the interval [0, maximum number in the list]\n","    self.priors = np.bincount(Y) / nb_examples\n","    print('Priors:')\n","    print(self.priors)\n","\n","    # We are calculating the number of occurrences of each word in each class.\n","    occs = np.zeros((self.nb_classes, self.nb_words))\n","    for i in range(nb_examples):\n","      c = Y[i]\n","      for w in range(self.nb_words):\n","        cnt = X[i][w]\n","        occs[c][w] += cnt\n","    print('Occurences:')\n","    print(occs)\n","\n","    # P(Word_i|Class) - likelihoods.\n","    self.like = np.zeros((self.nb_classes, self.nb_words))\n","    for c in range(self.nb_classes):\n","      for w in range(self.nb_words):\n","        up = occs[c][w] + self.pseudocount\n","        down = np.sum(occs[c]) + self.nb_words*self.pseudocount\n","        self.like[c][w] = up / down\n","    print('Likelihoods:')\n","    print(self.like)\n","\n","  def predict(self, bow):\n","    # P(Class|bow) for each class.\n","    probs = np.zeros(self.nb_classes)\n","    for c in range(self.nb_classes):\n","      prob = np.log(self.priors[c])\n","      for w in range(self.nb_words):\n","        cnt = bow[w]\n","        prob += cnt * np.log(self.like[c][w])\n","      probs[c] = prob\n","    # We are searching for the class with the highest probability.\n","    prediction = np.argmax(probs)\n","    return prediction\n","\n","  def predict_multiply(self, bow):\n","    # Calculating P(Class|bow) for each class\n","    # We multiply and exponentiate to compare the results with the slides.\n","    probs = np.zeros(self.nb_classes)\n","    for c in range(self.nb_classes):\n","      prob = self.priors[c]\n","      for w in range(self.nb_words):\n","        cnt = bow[w]\n","        prob *= self.like[c][w] ** cnt\n","      probs[c] = prob\n","    # We are finding the class with the highest probability.\n","    print('\\\"Probabilities\\\" for a test BoW (without log):')\n","    print(probs)\n","    prediction = np.argmax(probs)\n","    return prediction\n","\n","class_names = ['Pozitivno', 'Negativno']\n","\n","# train - 80%\n","training_x: np.ndarray = X[0, :(int(0.8 * X.shape[0]))]\n","training_y: np.ndarray = Y[0:(int(0.8 * len(Y)))]\n","\n","# test - 20%\n","test_x: np.ndarray = X[(int(0.8 * X.shape[0])):]\n","test_y: np.ndarray = Y[(int(0.8 * len(Y))):]\n","\n","# Fit the model\n","model = MultinomialNaiveBayes(nb_classes=2, nb_words=len(vocab), pseudocount=1)\n","model.fit(X, Y)\n","\n","nb_success = 0\n","for i in range(len(test_x)):\n","  vector = test_x[i]\n","  res = model.predict(np.asarray(vector))\n","  if res == test_y[i]:\n","    nb_success += 1\n","\n","print(\"Accuracy: \", nb_success/len(test_x))"]},{"cell_type":"markdown","metadata":{"id":"buog2zsnidj7"},"source":["## 4B\n","\n","Find the 5 most commonly used words in positive tweets. Do the same for negative tweets and comment on the results (in code comments). If we introduce the metric LR(word) as LR(word) = number of occurrences in positive tweets (word) / number of occurrences in negative tweets (word), find 5 words with the highest and 5 words with the lowest values of this metric. The metric is defined only for words that appear at least 10 times in positive and 10 times in negative corpus after data cleaning. Comment on the 10 obtained words, compare them with the previous results, and explain the meaning of the LR metric in the code comment below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1712144069972,"user":{"displayName":"Irina Tomic","userId":"15842450607275704514"},"user_tz":-120},"id":"nNYLYW89jI-j","outputId":"7f2b68fb-e784-4084-f9ea-2061afdf99e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["top 5 positive: [('like', (308, 103)), ('get', (223, 88)), ('amp', (209, 135)), ('new', (170, 56)), ('one', (137, 69))]\n","top 5 negative: [('fire', (90, 273)), ('bomb', (52, 186)), ('kill', (19, 161)), ('news', (57, 140)), ('amp', (209, 135))]\n","LR values\n","top 5 lr: ['full', 'love', 'obliter', 'scream', 'let']\n","low 5 lr: ['oil', 'report', 'train', 'warn', 'kill']\n"]}],"source":["# Positive tweet = non-disaster tweet (target = 0)\n","# Negative tweet = disaster tweet (target = 1)\n","\n","# dict_freq: key = unique word, value = tuple(freq in positive, freq in negative)\n","# dict_lr: key = unique word, value = lr value of word\n","\n","dict_freq = {}\n","dict_lr = {}\n","\n","# len(clean_corpus) = len(Y)\n","for i, tweet in enumerate(clean_corpus):\n","  for word in tweet:\n","    if word not in dict_freq:\n","        dict_freq[word] = (0, 0)\n","\n","    tweet_class = Y[i]\n","    if tweet_class == 0:\n","      dict_freq[word] = (dict_freq[word][0] + 1, dict_freq[word][1])       # positive\n","    else:\n","      dict_freq[word] = (dict_freq[word][0], dict_freq[word][1] + 1)       # negative\n","\n","# Sort the dict_req bas on positive and negative freq values\n","sorted_dict_freq_positive = sorted(dict_freq.items(), key=lambda x: x[1][0], reverse=True)\n","sorted_dict_freq_negative = sorted(dict_freq.items(), key=lambda x: x[1][1], reverse=True)\n","\n","top_5_positive = sorted_dict_freq_positive[:5]\n","top_5_negative = sorted_dict_freq_negative[:5]\n","\n","print(f'top 5 positive: {top_5_positive}')\n","print(f'top 5 negative: {top_5_negative}')\n","\n","for word, (pfreq, nfreq) in dict_freq.items():\n","    if pfreq >= 10 and nfreq >= 10:\n","        lr = pfreq / nfreq\n","        dict_lr[word] = lr\n","\n","sorted_lr = sorted(dict_lr.items(), key=lambda x: x[1], reverse=True)\n","\n","top_5_lr = [word for word, _ in sorted_lr[:5]]\n","low_5_lr = [word for word, _ in sorted_lr[-5:]]\n","\n","print('LR values')\n","print(f'top 5 lr: {top_5_lr}')\n","print(f'low 5 lr: {low_5_lr}')"]},{"cell_type":"markdown","metadata":{"id":"K_8EKnf4skou"},"source":["A higher LR (Likelihood Ratio) for a particular word indicates its greater relevance to a specific outcome. For example, a word that frequently appears in both negative and positive contexts may be equally important for both outcomes, meaning it does not determine whether the outcome is positive or negative.\n","\n","LR value is calculated as the ratio of occurrences in positive versus negative tweets. Therefore, a higher LR indicates a higher probability of a positive outcome for a tweet containing that word, while a lower LR indicates a higher probability of a negative outcome."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
